{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAAI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQy9G0okYsj"
      },
      "source": [
        "Various imports and downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmlikngWfnFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623c26e8-7d02-453e-f148-f265d7ea8d30"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "\n",
        "# increase the maxInt value\n",
        "maxInt = sys.maxsize\n",
        "while True:\n",
        "    try:\n",
        "        csv.field_size_limit(maxInt)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        maxInt = int(maxInt/10)\n",
        "\n",
        "import string\n",
        "import statistics\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "!pip install Unidecode\n",
        "import unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions used for text proecessing, file creation, and file updating. The majority of this utilizes the python regex library, csv writers and readers, and unidecode to convert the text to utf-8."
      ],
      "metadata": {
        "id": "PFaT_tqUm6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessAuthor(author, speakers, offices, discards, cap=True):\n",
        "    # get rid of ending whitespace\n",
        "    author = author.strip()\n",
        "\n",
        "    # check if not a talk\n",
        "    for d in discards:\n",
        "        if d.lower() in author.lower():\n",
        "            return \"\"\n",
        "\n",
        "    # get just author name without added titles\n",
        "    for o in offices:\n",
        "        if o in author:\n",
        "            regex = re.compile(\".*\"+o+\"[.]?\\s?\")\n",
        "            author = re.sub(regex, \"\", author)\n",
        "\n",
        "    # check if speaker has enough talks\n",
        "    if cap:\n",
        "        viableSpeaker = False\n",
        "        for s in speakers:\n",
        "            if s == author:\n",
        "                viableSpeaker = True\n",
        "                break\n",
        "        if not viableSpeaker:\n",
        "            return \"\"\n",
        "\n",
        "    # remove any ending punctuation if not an abbreviation like Jr.\n",
        "    author_lst = author.split()\n",
        "    if len(author_lst) > 1:\n",
        "        last = author_lst[-1]\n",
        "    elif len(author_lst) == 1:\n",
        "        last = author_lst[0]\n",
        "    else:\n",
        "        return \"\"\n",
        "    if len(last) > 3 and last[-1] in string.punctuation:\n",
        "        author = author[:len(author)-1]\n",
        "    # return the processed author name\n",
        "    return author\n",
        "\n",
        "\n",
        "def normalizeText(text):\n",
        "    # get rid of punctuation\n",
        "    nopunct = [char for char in text if char not in string.punctuation]\n",
        "    nopunct = \"\".join(nopunct)\n",
        "\n",
        "    # lemmatize\n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    a = \"\"\n",
        "    words = nopunct.split()\n",
        "    for i in range(len(words)):\n",
        "        b = lemmatiser.lemmatize(words[i], pos=\"v\")\n",
        "        b = b.lower()\n",
        "        a = a + b + \" \"\n",
        "\n",
        "    # remove Stopwords\n",
        "    words = [word for word in a.split() if word.lower()\n",
        "             not in stopwords.words('english')]\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessText(text, author, decade, year, TEXT_LEN_CAP, normalize=True):\n",
        "\n",
        "    words = text.split()\n",
        "    if len(words) < TEXT_LEN_CAP:\n",
        "        return \"\"\n",
        "\n",
        "    test_text = text[:600]\n",
        "\n",
        "    change_made = False\n",
        "\n",
        "    # change the text to a normal utf-8 format\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    # get rid of extra info before actual talk\n",
        "    regex = re.compile(r\"^COMMENTS. \")\n",
        "    match_obj = re.search(regex, test_text[:50])\n",
        "    if match_obj != None:\n",
        "        change_made = True\n",
        "        span = match_obj.span()\n",
        "        text = text[span[1]:]\n",
        "\n",
        "    regex = re.compile(r\"^REMARKS. \")\n",
        "    match_obj = re.search(regex, test_text[:50])\n",
        "    if match_obj != None:\n",
        "        change_made = True\n",
        "        span = match_obj.span()\n",
        "        text = text[span[1]:]\n",
        "\n",
        "    dec_int = int(decade[:len(decade)-1])\n",
        "    year = 0 if year == \"\" else int(year)\n",
        "    if dec_int <= 1880 and (year == \"\" or year < 1880):\n",
        "        regex = re.compile(rf\"(R|r)eported by .*?  \")\n",
        "        match_obj = re.search(regex, test_text)\n",
        "        if match_obj != None:\n",
        "            change_made = True\n",
        "            span = match_obj.span()\n",
        "            text = text[span[1]:]\n",
        "\n",
        "    elif year >= 1971:\n",
        "        regex = re.compile(r\"(W|w)atch, listen, download.\\s*\")\n",
        "        match_obj = re.search(regex, test_text)\n",
        "        if match_obj != None:\n",
        "            change_made = True\n",
        "            span = match_obj.span()\n",
        "            text = text[span[1]:]\n",
        "\n",
        "    else:\n",
        "        regex = re.compile(r\".*as follows:  \")\n",
        "        match_obj = re.search(regex, test_text)\n",
        "        if match_obj != None:\n",
        "            change_made = True\n",
        "            span = match_obj.span()\n",
        "            text = text[span[1]:]\n",
        "\n",
        "        author = author.lower()\n",
        "        counselors = \"|((F|f)irst)|((S|s)econd) counselor\"\n",
        "        remarks = \"|((O|o)pening)|((C|c)losing) remarks\"\n",
        "        options = \"(O|o)f the |(A|a)ssistant\" + counselors + \\\n",
        "            \"|(P|p)resid\" + remarks + \"|(P|p)atriarch\"\n",
        "        regex = re.compile(\n",
        "            rf\"({author}(:|\\.)?  (?!{options}))|({author}(:|\\.)?  .*?  )\")\n",
        "        match_obj = re.search(regex, test_text.lower())\n",
        "        if match_obj != None:\n",
        "            change_made = True\n",
        "            span = match_obj.span()\n",
        "            text = text[span[1]:]\n",
        "\n",
        "    # if after removing extra info, too small, reject document\n",
        "    words = text.split()\n",
        "    if len(words) < TEXT_LEN_CAP:\n",
        "        return \"\"\n",
        "\n",
        "    # if no change made, not in a good enough format to use\n",
        "    if not change_made:\n",
        "        return \"\"\n",
        "\n",
        "    # change hyphens to spaces\n",
        "    text = re.sub(\"-\", \" \", text)\n",
        "\n",
        "    # create tokens that represent open quote and close quote\n",
        "    text = re.sub(r'\\s+\"', \" QUOTEOPEN \", text)\n",
        "    text = re.sub(r'\"\\s+', \" QUOTECLOSE \", text)\n",
        "\n",
        "    # replace all scripture references with token \"SCRIPREF\"\n",
        "    regex1 = re.compile(r\"(\\([^\\)]*?:[^\\)]*?\\))|(\\[[^\\]]*?:[^\\]]*?\\])\")\n",
        "    text = re.sub(regex1, \" SCRIPREF \", text)\n",
        "\n",
        "    # get rid of all other paranthetical extras\n",
        "    text = re.sub(r\"\\([^\\)]*?\\)\", \" \", text)\n",
        "\n",
        "    # get rid of stopwords and punctuation, lemmatize the words\n",
        "    if normalize:\n",
        "        text = normalizeText(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def find_text_cap():\n",
        "    text_lengths.sort()\n",
        "    med = statistics.median(text_lengths)\n",
        "    sd = statistics.stdev(text_lengths)\n",
        "    print(med)\n",
        "    print(sd)\n",
        "    print(med-sd)\n",
        "\n",
        "    from matplotlib import pyplot as plt\n",
        "\n",
        "    print(len(text_lengths))\n",
        "    plt.plot(text_lengths[:6000])\n",
        "\n",
        "    Q1 = np.percentile(text_lengths, 25, interpolation='midpoint')\n",
        "    Q3 = np.percentile(text_lengths, 75, interpolation='midpoint')\n",
        "    IQR = Q3 - Q1\n",
        "    print(\"Q1: \", Q1)\n",
        "    print(\"Q3: \", Q3)\n",
        "    print(\"IQR: \", IQR)\n",
        "    k = 1.5\n",
        "    lower_cap = Q1-k*IQR\n",
        "    arr = np.array(text_lengths)\n",
        "    small = arr[arr < 1578.5]\n",
        "    small_q1 = np.percentile(small, 25, interpolation=\"midpoint\")\n",
        "    print(small_q1)\n",
        "\n",
        "\n",
        "def initFile():\n",
        "    # if it doesn't already exist create the fil; otherwise, clear the file\n",
        "    file = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "\n",
        "    # add headers to the file\n",
        "    with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"r+\") as file:\n",
        "        writer = csv.writer(file, delimiter=\"\\t\")\n",
        "        response = [\"author\", \"text\"]\n",
        "        writer.writerow(response)\n",
        "\n",
        "\n",
        "def addDocToFile(author, text):\n",
        "    # add a row to the newTexts.tsv file with the author and talk\n",
        "    with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"a+\") as file:\n",
        "        writer = csv.writer(file, delimiter=\"\\t\")\n",
        "        lst = []\n",
        "        lst.append(author)\n",
        "        lst.append(text)\n",
        "        writer.writerow(lst)\n",
        "\n",
        "\n",
        "def writeToFile(authors, texts):\n",
        "    # open/create/clear file\n",
        "    file = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "\n",
        "    # add all authors and texts to the file\n",
        "    with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"r+\") as file:\n",
        "        writer = csv.writer(file, delimiter=\"\\t\")\n",
        "        response = [\"author\", \"text\"]\n",
        "        writer.writerow(response)\n",
        "        for i in range(len(authors)):\n",
        "            lst = []\n",
        "            lst.append(authors[i])\n",
        "            lst.append(texts[i])\n",
        "            writer.writerow(lst)\n"
      ],
      "metadata": {
        "id": "H7Rc0Kb-lO3c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the new preprocessed corpus that will be used to train the sklearn multinomial naive bayes classifier. A portion of this code also extracts all of the speakers in the corpus and stores this list in a file. This file is later used to explore how author document count affects the overall model accuracy. This code only needs to be run once, since it saves the processed text in Google Drive.\n",
        "\n",
        "This preprocessing took many hours, so it has been restructured to update the files progressively and in segments if needed."
      ],
      "metadata": {
        "id": "L56ApZVVnRBZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FAhvEzJkcjz"
      },
      "source": [
        "# open the file\n",
        "f = open(\"/content/drive/MyDrive/NLP/restructured-conference.tsv\")\n",
        "f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "# get headers\n",
        "headers = next(f_reader)\n",
        "# discard the open line\n",
        "next(f_reader)\n",
        "\n",
        "TEXT_LEN_CAP = 600\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLP/speakers.txt\") as file:\n",
        "    speakers = [s.strip() for s in file.readlines()]\n",
        "with open(\"/content/drive/MyDrive/NLP/titles.txt\") as file:\n",
        "    offices = [o.strip() for o in file.readlines()]\n",
        "with open(\"/content/drive/MyDrive/NLP/discards.txt\") as file:\n",
        "    discards = [d.strip() for d in file.readlines()]\n",
        "\n",
        "texts = []\n",
        "authors = []\n",
        "author_texts = {}\n",
        "authors_set = set()\n",
        "text_lengths = []\n",
        "\n",
        "DOC_INDEX_START = 0\n",
        "\n",
        "\n",
        "def init_speaker_count():\n",
        "    file = open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "\n",
        "    '''\n",
        "    file = open(\"/content/drive/MyDrive/NLP/speakers.txt\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "    '''\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    file.close()\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"r+\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        response = [\"speaker\", \"count\"]\n",
        "        writer.writerow(response)\n",
        "\n",
        "\n",
        "def add_speaker(speaker: str):\n",
        "    with open(\"/content/drive/MyDrive/NLP/speakers.txt\", \"a+\") as file:\n",
        "        file.write(speaker + \"\\n\")\n",
        "\n",
        "\n",
        "def get_speaker_count():\n",
        "    speaker_counts = {}\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\") as file:\n",
        "        file_reader = csv.reader(file)\n",
        "        next(file_reader)\n",
        "        for row in file_reader:\n",
        "            s = row[0]\n",
        "            c = row[1]\n",
        "\n",
        "            speaker_counts[s] = int(c)\n",
        "\n",
        "    return speaker_counts\n",
        "\n",
        "\n",
        "def create_author_doc_count_file():\n",
        "    speaker_counts = {}\n",
        "    with open(\"/content/drive/MyDrive/NLP/speakers.txt\") as file:\n",
        "        speakers = [speaker.strip() for speaker in file.readlines()]\n",
        "        for s in speakers:\n",
        "            if s not in speaker_counts:\n",
        "                speaker_counts[s] = 1\n",
        "            else:\n",
        "                speaker_counts[s] += 1\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"r+\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        for s in speaker_counts:\n",
        "            response = [s, speaker_counts[s]]\n",
        "            writer.writerow(response)\n",
        "\n",
        "\n",
        "def create_speaker_file():\n",
        "    f = open(\"/content/drive/MyDrive/NLP/restructured-conference.tsv\")\n",
        "    f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "    # get headers\n",
        "    headers = next(f_reader)\n",
        "    # discard the open line\n",
        "    next(f_reader)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/docIndex.txt\") as file:\n",
        "        DOC_INDEX_START = file.read()\n",
        "        print(f\"|{DOC_INDEX_START}|\")\n",
        "        if DOC_INDEX_START == \"\":\n",
        "            DOC_INDEX_START = 0\n",
        "        else:\n",
        "            DOC_INDEX_START = int(DOC_INDEX_START)\n",
        "\n",
        "    doc_index = 0\n",
        "\n",
        "    for document in f_reader:\n",
        "        decade = document[0]\n",
        "        year = document[1]\n",
        "        month = document[2]\n",
        "        session_num = document[3]\n",
        "        author = document[4]\n",
        "        title = document[5]\n",
        "        gender = document[6]\n",
        "        role = document[7]\n",
        "        subjects = document[8]\n",
        "        text = document[9]\n",
        "\n",
        "        doc_index += 1\n",
        "\n",
        "        # startup where it left off\n",
        "        if doc_index <= DOC_INDEX_START:\n",
        "            continue\n",
        "\n",
        "        author = preprocessAuthor(\n",
        "            author, speakers, offices, discards, cap=False)\n",
        "        if author == \"\":\n",
        "            continue\n",
        "        cpy = text\n",
        "        text = preprocessText(text, author, decade, year,\n",
        "                              TEXT_LEN_CAP, normalize=False)\n",
        "        if text == \"\":\n",
        "            continue\n",
        "\n",
        "        add_speaker(author)\n",
        "        #print(\"doc:\", doc_index, \"was added\")\n",
        "        with open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\") as file:\n",
        "            file.write(str(doc_index))\n",
        "\n",
        "\n",
        "def select_authors_and_docs():\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/docIndex.txt\") as file:\n",
        "        DOC_INDEX_START = file.read()\n",
        "        print(f\"|{DOC_INDEX_START}|\")\n",
        "        if DOC_INDEX_START == \"\":\n",
        "            DOC_INDEX_START = 0\n",
        "        else:\n",
        "            DOC_INDEX_START = int(DOC_INDEX_START)\n",
        "\n",
        "    print(\"Start doc: \", DOC_INDEX_START)\n",
        "\n",
        "    doc_index = 0\n",
        "\n",
        "    for document in f_reader:\n",
        "        decade = document[0]\n",
        "        year = document[1]\n",
        "        month = document[2]\n",
        "        session_num = document[3]\n",
        "        author = document[4]\n",
        "        title = document[5]\n",
        "        gender = document[6]\n",
        "        role = document[7]\n",
        "        subjects = document[8]\n",
        "        text = document[9]\n",
        "\n",
        "        doc_index += 1\n",
        "\n",
        "        # startup where it left off\n",
        "        if doc_index <= DOC_INDEX_START:\n",
        "            continue\n",
        "\n",
        "        author = preprocessAuthor(author, speakers, offices, discards)\n",
        "        if author == \"\":\n",
        "            continue\n",
        "        cpy = text\n",
        "        text = preprocessText(text, author, decade, year, TEXT_LEN_CAP)\n",
        "        if text == \"\":\n",
        "            continue\n",
        "\n",
        "        authors.append(author)\n",
        "        texts.append(text)\n",
        "        if author not in author_texts:\n",
        "            author_texts[author] = [text]\n",
        "            text_lengths.append(len(text.split()))\n",
        "        else:\n",
        "            author_texts[author].append(text)\n",
        "            text_lengths.append(len(text.split()))\n",
        "\n",
        "        addDocToFile(author, text)\n",
        "        #print(\"doc:\", doc_index, \"was added\")\n",
        "        with open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\") as file:\n",
        "            file.write(str(doc_index))\n",
        "\n",
        "\n",
        "def get_speakers(cap=30):\n",
        "    speaker_counts = get_speaker_count()\n",
        "    speakers = []\n",
        "    for s in speaker_counts:\n",
        "        if speaker_counts[s] >= cap:\n",
        "            speakers.append(s)\n",
        "    return speakers\n",
        "\n",
        "# init_speaker_count()\n",
        "# create_author_doc_count_file()\n",
        "#speaker_counts = get_speaker_count()\n",
        "\n",
        "# initFile()\n",
        "# select_authors_and_docs()\n",
        "# timePreprocess()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Multinomial Naive Bayes classifier on the preprocessed text"
      ],
      "metadata": {
        "id": "3c1_2aSCrIWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and downloads\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# download these if skipping the previous cells in the notebook\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "PCoU_-zwwzPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a2c55a-92e9-441e-fe54-c4bedaf9db92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to get the necessary authors and texts lists to train the model. These functions are utilized to change up what authors, texts, and the preprocessing of the texts for each model test."
      ],
      "metadata": {
        "id": "qZLHEGtSn6bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAuthorsAndTexts():\n",
        "    f = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\")\n",
        "    f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "    # get headers\n",
        "    headers = next(f_reader)\n",
        "\n",
        "    authors = []\n",
        "    texts = []\n",
        "    author_text_dict = {}\n",
        "    author_doc_count = {}\n",
        "    author_total_words = {}\n",
        "\n",
        "    for document in f_reader:\n",
        "        author = document[0]\n",
        "        text = document[1]\n",
        "        authors.append(author)\n",
        "        texts.append(text)\n",
        "\n",
        "        if author not in author_text_dict:\n",
        "            author_text_dict[author] = [text]\n",
        "            author_doc_count[author] = 1\n",
        "            author_total_words[author] = len(text.split())\n",
        "        else:\n",
        "            author_text_dict[author].append(text)\n",
        "            author_doc_count[author] += 1\n",
        "            author_total_words[author] += len(text.split())\n",
        "\n",
        "    return authors, texts, author_text_dict, author_doc_count, author_total_words\n",
        "\n",
        "def text_process(text):\n",
        "    return [word for word in text.split()]\n",
        "\n",
        "def text_process_discards(text, discards=None):\n",
        "    discards = [\"quoteopen\", \"quoteclose\", \"scripref\", \"lord\", \"god\", \"say\"] if discards == None else discards\n",
        "    return [word for word in text.split() if word not in discards]\n",
        "\n",
        "def discard(texts, discards):\n",
        "    for i in range(len(texts)):\n",
        "        text = texts[i]\n",
        "        text = \" \".join([word for word in text.split() if word not in discards])\n",
        "        texts[i] = text\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "XEVMK7FywwJi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code used to train the multinomial naive bayes model and run various tests. The tests focus on how the document cap (getting rid of authors who have fewer documents attributed to them), the number of authors trained together (batch size), and how features like quotes and scripture references affect the overall model accuracy."
      ],
      "metadata": {
        "id": "1WdSv7GtoNOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import exit\n",
        "from numpy.testing import assert_equal\n",
        "from statistics import mean\n",
        "from copy import deepcopy\n",
        "from random import randint\n",
        "from time import time\n",
        "\n",
        "authors, texts, author_texts, author_doc_count, author_total_words = getAuthorsAndTexts()\n",
        "\n",
        "def filter_by_authors(auth_lst):\n",
        "    global authors\n",
        "    global texts\n",
        "    global author_texts\n",
        "    global author_doc_count\n",
        "    global author_total_words\n",
        "\n",
        "    authors_new = []\n",
        "    texts_new = []\n",
        "    auths = set()\n",
        "    for i, a in enumerate(authors):\n",
        "        if a in auth_lst:\n",
        "            authors_new.append(a)\n",
        "            texts_new.append(texts[i])\n",
        "            auths.add(a)\n",
        "\n",
        "    return authors_new, texts_new, len(auths)\n",
        "\n",
        "\n",
        "def filter_author_count(count=50):\n",
        "    global authors\n",
        "    global texts\n",
        "    global author_texts\n",
        "    global author_doc_count\n",
        "    global author_total_words\n",
        "\n",
        "    authors_new = []\n",
        "    texts_new = []\n",
        "    auths = set()\n",
        "    for i, a in enumerate(authors):\n",
        "        if author_doc_count[a] > count:\n",
        "            authors_new.append(a)\n",
        "            texts_new.append(texts[i])\n",
        "            auths.add(a)\n",
        "\n",
        "    return authors_new, texts_new, len(auths)\n",
        "\n",
        "\n",
        "def mnb_model(authors, texts, discards=[], print_report=True):\n",
        "\n",
        "    authors = deepcopy(authors)\n",
        "    texts = deepcopy(texts)\n",
        "\n",
        "    labelencoder = LabelEncoder()\n",
        "    y = labelencoder.fit_transform(authors)\n",
        "    authors_reverse = labelencoder.inverse_transform(y)\n",
        "    labels = list(labelencoder.classes_)\n",
        "\n",
        "    y_counts = {}\n",
        "    for e in y:\n",
        "        if e not in y_counts:\n",
        "            y_counts[e] = 1\n",
        "        else:\n",
        "            y_counts[e] += 1\n",
        "\n",
        "    author_word_count = {}\n",
        "    for i, a in enumerate(authors):\n",
        "        words = len(texts[i].split())\n",
        "        if a not in author_word_count:\n",
        "            author_word_count[a] = words\n",
        "        else:\n",
        "            author_word_count[a] += words\n",
        "\n",
        "    texts = discard(texts, discards)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        texts, y, test_size=0.2)\n",
        "\n",
        "    bow_transformer = CountVectorizer().fit(X_train)\n",
        "    text_bow_train = bow_transformer.transform(X_train)\n",
        "    text_bow_test = bow_transformer.transform(X_test)\n",
        "\n",
        "    model = MultinomialNB()\n",
        "    model = model.fit(text_bow_train, y_train)\n",
        "\n",
        "    model.score(text_bow_train, y_train)\n",
        "    model.score(text_bow_test, y_test)\n",
        "\n",
        "    predictions = model.predict(text_bow_test)\n",
        "    report = classification_report(\n",
        "        y_test, predictions, zero_division=0, output_dict=True)\n",
        "    report_str = classification_report(\n",
        "        y_test, predictions, zero_division=0, output_dict=False)\n",
        "    if print_report:\n",
        "        min = np.inf\n",
        "        max = 0\n",
        "        mins = []\n",
        "        wpds = []\n",
        "        print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "            \"\", \"precision\", \"recall\", \"f1-score\", \"documents\", \"words\", \"wpd\"))\n",
        "        for item in report:\n",
        "            # print(item)\n",
        "            if item == \"accuracy\":\n",
        "                print()\n",
        "                l = \"accuracy\"\n",
        "                p = \" \"\n",
        "                r = \" \"\n",
        "                f = round(report[item], 2)\n",
        "                d = \" \"\n",
        "                w = \" \"\n",
        "                wpd = \" \"\n",
        "            elif item == \"micro avg\":\n",
        "                l = \"micro avg\"\n",
        "                p = round(report[item][\"precision\"], 2)\n",
        "                r = round(report[item][\"recall\"], 2)\n",
        "                f = round(report[item][\"f1-score\"], 2)\n",
        "                d = \" \"\n",
        "                w = \" \"\n",
        "                wpd = \" \"\n",
        "            elif item == \"weighted avg\":\n",
        "                l = \"weighted avg\"\n",
        "                p = round(report[item][\"precision\"], 2)\n",
        "                r = round(report[item][\"recall\"], 2)\n",
        "                f = round(report[item][\"f1-score\"], 2)\n",
        "                d = \" \"\n",
        "                w = \" \"\n",
        "                wpd = \" \"\n",
        "            elif item == \"macro avg\":\n",
        "                l = \"macro avg\"\n",
        "                p = round(report[item][\"precision\"], 2)\n",
        "                r = round(report[item][\"recall\"], 2)\n",
        "                f = round(report[item][\"f1-score\"], 2)\n",
        "                d = \" \"\n",
        "                w = \" \"\n",
        "                wpd = \" \"\n",
        "            else:\n",
        "                l = labels[int(item)]\n",
        "                p = round(report[item][\"precision\"], 2)\n",
        "                r = round(report[item][\"recall\"], 2)\n",
        "                f = round(report[item][\"f1-score\"], 2)\n",
        "                d = y_counts[int(item)]\n",
        "                w = author_word_count[l]\n",
        "                wpd = w/d\n",
        "\n",
        "                if f == 0.0 and d > max:\n",
        "                    max = d\n",
        "                if f > 0 and d < min:\n",
        "                    min = d\n",
        "\n",
        "                wpds.append(w/d)\n",
        "\n",
        "            print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                l, p, r, f, d, w, wpd))\n",
        "        print(\"minimum documents to produce results was min: {} max: {}\".format(min, max))\n",
        "        print(\"average word per doc: {}\".format(mean(wpds)))\n",
        "\n",
        "        # print(report_str)\n",
        "\n",
        "    results = {}\n",
        "    results[\"model\"] = model\n",
        "    results[\"labels\"] = labels\n",
        "    results[\"y_counts\"] = y_counts\n",
        "    results[\"author_word_count\"] = author_word_count\n",
        "\n",
        "    return results, report\n",
        "\n",
        "\n",
        "def run_cap_test(discards=[], label=\"\"):\n",
        "    doc_caps, accuracies, author_counts, remainings = [], [], [], []\n",
        "\n",
        "    authors_set = set()\n",
        "    prev_authors_set = set()\n",
        "\n",
        "    for iteration in range(10):\n",
        "        print(\"iteration {} / {}\".format(iteration+1, 10), end=\" : \")\n",
        "        start = time()\n",
        "        for i in range(25, 100, 5):\n",
        "            auths_test, texts_test, auths = filter_author_count(i)\n",
        "            authors_set = set(auths_test)\n",
        "            m, r = mnb_model(auths_test, texts_test,\n",
        "                             discards=discards, print_report=False)\n",
        "            remaining = prev_authors_set - authors_set\n",
        "            doc_caps.append(i)\n",
        "            accuracies.append(r[\"accuracy\"])\n",
        "            author_counts.append(auths)\n",
        "            remainings.append(remaining)\n",
        "            prev_authors_set = deepcopy(authors_set)\n",
        "\n",
        "            #print(i, \" : \", r[\"accuracy\"], \" : \", auths, \" : \", remaining)\n",
        "        end = time()\n",
        "        print(\"it took {} minutes\".format((end-start)/60))\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/NLP/doc_acc_relation\" +\n",
        "                str(label)+\".csv\", \"w+\")\n",
        "    file.close()\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/NLP/doc_acc_relation\"+str(label)+\".csv\", \"r+\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        header = [\"document min\", \"accuracy\",\n",
        "                  \"number of valid authors\", \"discarded authors\"]\n",
        "        writer.writerow(header)\n",
        "        for i in range(len(doc_caps)):\n",
        "            cap = doc_caps[i]\n",
        "            acc = accuracies[i]\n",
        "            auth = author_counts[i]\n",
        "            rem = remainings[i]\n",
        "\n",
        "            response = [cap, acc, auth, rem]\n",
        "            writer.writerow(response)\n",
        "\n",
        "\n",
        "def author_amount_test(discards=[], label=\"\"):\n",
        "    lst = []\n",
        "    with open(\"/content/drive/MyDrive/NLP/doc_acc_relation\"+str(label)+\".csv\") as file:\n",
        "        f_reader = csv.reader(file)\n",
        "        next(f_reader)\n",
        "        for line in f_reader:\n",
        "            text = line[3]\n",
        "            if text != \"set()\":\n",
        "                lst = lst + text[2:len(text)-2].split(\"', '\")\n",
        "    # print(lst)\n",
        "    lst = list(set(lst))\n",
        "    #lst = lst[:len(lst)-2]\n",
        "    # print(len(lst))\n",
        "\n",
        "    rows = []\n",
        "    for iteration in range(10):\n",
        "        print(\"iteration {} / {}\".format(iteration+1, 10), end=\" : \")\n",
        "        start = time()\n",
        "        rows = []\n",
        "        for size in range(4, len(lst), 4):\n",
        "            batches = []\n",
        "            total = 0\n",
        "            author_bank = deepcopy(lst)\n",
        "            counter = 0\n",
        "            batch = []\n",
        "            while len(author_bank) > 0:\n",
        "                a = author_bank.pop(randint(0, len(author_bank)-1))\n",
        "                batch.append(a)\n",
        "                counter += 1\n",
        "                if counter >= size:\n",
        "                    counter = 0\n",
        "                    batches.append(batch)\n",
        "                    batch = []\n",
        "\n",
        "            counts, accuracies, dpa = [], [], []\n",
        "\n",
        "            accuracies = []\n",
        "            reg_size = len(batches[0])\n",
        "            for b in batches:\n",
        "                if len(b) == reg_size:\n",
        "                    authors_batch, texts_batch, authors_count = filter_by_authors(b)\n",
        "                    m, r = mnb_model(authors_batch, texts_batch,\n",
        "                                     discards=discards, print_report=False)\n",
        "\n",
        "                    counts.append(authors_count)\n",
        "                    accuracies.append(r[\"accuracy\"])\n",
        "                    dpa.append(len(texts_batch)/authors_count)\n",
        "\n",
        "                    rows.append([authors_count, r[\"accuracy\"],\n",
        "                                len(texts_batch)/authors_count])\n",
        "                    #print(authors_count, \" : \", r[\"accuracy\"], \" : \", len(texts_batch)/authors_count, \" : \", set(authors_batch))\n",
        "        end = time()\n",
        "        print(\"it took {} minutes\".format((end-start)/60))\n",
        "        with open(\"/content/drive/MyDrive/NLP/auth_count_acc_relation\"+str(label)+\".csv\", \"a+\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            for r in rows:\n",
        "                writer.writerow(r)\n",
        "\n",
        "\n",
        "def discards_test():\n",
        "    discards = [[\"quoteopen\", \"quoteclose\", \"scripref\"], [\n",
        "        \"quoteopen\", \"quoteclose\"], [\"scripref\"], [], [\"quoteclose\"]]\n",
        "    labels = [\"normal\", \"ref\", \"quotes\", \"ref_quotes\", \"quote_open\"]\n",
        "\n",
        "    for i, d in enumerate(discards):\n",
        "        print(\"DISCARD NUMBER {}\".format(i))\n",
        "        with open(\"/content/drive/MyDrive/NLP/auth_count_acc_relation\"+str(\"_\"+labels[i])+\".csv\", \"w+\") as file:\n",
        "            file.truncate(0)\n",
        "        with open(\"/content/drive/MyDrive/NLP/auth_count_acc_relation\"+str(\"_\"+labels[i])+\".csv\", \"r+\") as file:\n",
        "            writer = csv.writer(file)\n",
        "            header = [\"author count\", \"accuracy\", \"avg doc_per_author\"]\n",
        "            writer.writerow(header)\n",
        "\n",
        "        authors, texts, auths_count = filter_author_count(60)\n",
        "        #run_cap_test(d, str(\"_\"+labels[i]))\n",
        "        author_amount_test(d, str(\"_\"+labels[i]))\n",
        "\n",
        "\n",
        "def write_report_to_file(results, report):\n",
        "\n",
        "    y_counts = results[\"y_counts\"]\n",
        "    author_word_count = results[\"author_word_count\"]\n",
        "    labels = results[\"labels\"]\n",
        "\n",
        "    min = np.inf\n",
        "    max = 0\n",
        "    mins = []\n",
        "    wpds = []\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/NLP/model_results.csv\", \"w+\")\n",
        "    file.truncate(0)\n",
        "    writer = csv.writer(file)\n",
        "    header = [\"author\", \"precision\", \"recall\",\n",
        "              \"f1-score\", \"documents\", \"words\", \"wpd\"]\n",
        "    writer.writerow(header)\n",
        "\n",
        "    print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "        \"\", \"precision\", \"recall\", \"f1-score\", \"documents\", \"words\", \"wpd\"))\n",
        "    for item in report:\n",
        "        # print(item)\n",
        "        if item == \"accuracy\":\n",
        "            print()\n",
        "            l = \"accuracy\"\n",
        "            p = \" \"\n",
        "            r = \" \"\n",
        "            f = round(report[item], 2)\n",
        "            d = \" \"\n",
        "            w = \" \"\n",
        "            wpd = \" \"\n",
        "        elif item == \"micro avg\":\n",
        "            l = \"micro avg\"\n",
        "            p = round(report[item][\"precision\"], 2)\n",
        "            r = round(report[item][\"recall\"], 2)\n",
        "            f = round(report[item][\"f1-score\"], 2)\n",
        "            d = \" \"\n",
        "            w = \" \"\n",
        "            wpd = \" \"\n",
        "        elif item == \"weighted avg\":\n",
        "            l = \"weighted avg\"\n",
        "            p = round(report[item][\"precision\"], 2)\n",
        "            r = round(report[item][\"recall\"], 2)\n",
        "            f = round(report[item][\"f1-score\"], 2)\n",
        "            d = \" \"\n",
        "            w = \" \"\n",
        "            wpd = \" \"\n",
        "        elif item == \"macro avg\":\n",
        "            l = \"macro avg\"\n",
        "            p = round(report[item][\"precision\"], 2)\n",
        "            r = round(report[item][\"recall\"], 2)\n",
        "            f = round(report[item][\"f1-score\"], 2)\n",
        "            d = \" \"\n",
        "            w = \" \"\n",
        "            wpd = \" \"\n",
        "        else:\n",
        "            l = labels[int(item)]\n",
        "            p = round(report[item][\"precision\"], 2)\n",
        "            r = round(report[item][\"recall\"], 2)\n",
        "            f = round(report[item][\"f1-score\"], 2)\n",
        "            d = y_counts[int(item)]\n",
        "            w = author_word_count[l]\n",
        "            wpd = w/d\n",
        "\n",
        "            if f == 0.0 and d > max:\n",
        "                max = d\n",
        "            if f > 0 and d < min:\n",
        "                min = d\n",
        "\n",
        "            wpds.append(w/d)\n",
        "\n",
        "        print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "            l, p, r, f, d, w, wpd))\n",
        "        writer.writerow([l, p, r, f, d, w, wpd])\n",
        "\n",
        "    file.close()\n",
        "    print(\"minimum documents to produce results was min: {} max: {}\".format(min, max))\n",
        "    print(\"average word per doc: {}\".format(mean(wpds)))\n",
        "\n",
        "\n",
        "def run_mnb_model():\n",
        "    authors, texts, auths_count = filter_author_count(100)\n",
        "    results, report = mnb_model(authors, texts, discards=[\n",
        "                                \"quoteclose\"], print_report=False)\n",
        "    write_report_to_file(results, report)\n",
        "\n",
        "\n",
        "def table_for_paper():\n",
        "    file = open(\"/content/drive/MyDrive/NLP/model_results.csv\")\n",
        "    f_reader = csv.reader(file)\n",
        "    next(f_reader)\n",
        "    for line in f_reader:\n",
        "        string = \"\\\\verb|{}| & \\\\verb|{}| & \\\\verb|{}| & \\\\verb|{}| & \\\\verb|{}| \\\\\\\\\"\\\n",
        "            .format(line[0], line[1], line[2], line[3], line[4])\n",
        "        print(string)\n",
        "\n",
        "\n",
        "run_mnb_model()\n",
        "# table_for_paper()\n",
        "# discards_test()\n",
        "# author_amount_test()\n",
        "# run_cap_test()\n",
        "#mnb_model(authors, texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGRInv6Z8EoG",
        "outputId": "121f2a91-bef6-44bf-a3ff-78bc95c2adbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision  recall     f1-score   documents  words      wpd       \n",
            "Boyd K. Packer            1.0        0.56       0.71       107        113221     1058.1401869158879\n",
            "Brigham Young             0.87       0.95       0.9        242        638475     2638.3264462809916\n",
            "David O. McKay            0.92       1.0        0.96       180        202131     1122.95   \n",
            "Ezra Taft Benson          0.83       0.75       0.79       112        141063     1259.4910714285713\n",
            "George Q. Cannon          0.64       0.88       0.74       140        315889     2256.35   \n",
            "Gordon B. Hinckley        0.61       1.0        0.76       234        235563     1006.6794871794872\n",
            "Heber J. Grant            0.8        0.84       0.82       112        182827     1632.3839285714287\n",
            "James E. Faust            1.0        0.38       0.55       103        113788     1104.7378640776699\n",
            "John Taylor               0.92       0.96       0.94       155        364000     2348.3870967741937\n",
            "Joseph F. Smith           0.77       0.62       0.69       137        180791     1319.6423357664235\n",
            "Joseph Fielding Smith     1.0        0.6        0.75       110        90556      823.2363636363636\n",
            "Marion G. Romney          0.84       0.84       0.84       118        130299     1104.2288135593221\n",
            "Orson Pratt               0.81       0.96       0.88       115        365298     3176.504347826087\n",
            "Spencer W. Kimball        0.94       0.59       0.72       119        165714     1392.5546218487395\n",
            "Thomas S. Monson          0.9        0.98       0.94       217        237939     1096.4930875576038\n",
            "Wilford Woodruff          0.94       0.71       0.81       105        156509     1490.5619047619048\n",
            "\n",
            "accuracy                                        0.82                                       \n",
            "macro avg                 0.86       0.79       0.8                                        \n",
            "weighted avg              0.85       0.82       0.82                                       \n",
            "minimum documents to produce results was min: 103 max: 0\n",
            "average word per doc: 1551.9167222615422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analsysis and plotting of each of the tests"
      ],
      "metadata": {
        "id": "sN8qEk82E4QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "!pip install folium==0.2.1\n",
        "!pip install pdflatex\n",
        "!sudo apt-get install texlive-latex-recommended \n",
        "!sudo apt install texlive-latex-extra\n",
        "!sudo apt install dvipng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lROXk5k_BEM6",
        "outputId": "b69eb7e6-bf51-4c38-ae1c-1a048ab057ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: folium==0.2.1 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: pdflatex in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: attrs<19.0,>=18.2 in /usr/local/lib/python3.7/dist-packages (from pdflatex) (18.2.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "texlive-latex-recommended is already the newest version (2017.20180305-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "texlive-latex-extra is already the newest version (2017.20180305-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "dvipng is already the newest version (1.15-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 455.24408\n",
        "def set_size(width_pt, fraction=1, subplots=(1, 1)):\n",
        "    \"\"\"Set figure dimensions to sit nicely in our document.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    width_pt: float\n",
        "            Document width in points\n",
        "    fraction: float, optional\n",
        "            Fraction of the width which you wish the figure to occupy\n",
        "    subplots: array-like, optional\n",
        "            The number of rows and columns of subplots.\n",
        "    Returns\n",
        "    -------\n",
        "    fig_dim: tuple\n",
        "            Dimensions of figure in inches\n",
        "    \"\"\"\n",
        "    # Width of figure (in pts)\n",
        "    fig_width_pt = width_pt * fraction\n",
        "    # Convert from pt to inches\n",
        "    inches_per_pt = 1 / 72.27\n",
        "\n",
        "    # Golden ratio to set aesthetic figure height\n",
        "    golden_ratio = (5**.5 - 1) / 2\n",
        "\n",
        "    # Figure width in inches\n",
        "    fig_width_in = fig_width_pt * inches_per_pt\n",
        "    # Figure height in inches\n",
        "    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n",
        "\n",
        "    return (fig_width_in, fig_height_in)\n",
        "\n",
        "\n",
        "def export_plot(plt, name, full_address=False):\n",
        "    matplotlib.use(\"pgf\")\n",
        "    matplotlib.rcParams.update({\n",
        "        \"pgf.texsystem\": \"pdflatex\",\n",
        "        'font.family': 'serif',\n",
        "        'text.usetex': True,\n",
        "        'pgf.rcfonts': False,\n",
        "    })\n",
        "    if full_address:\n",
        "        filename = name + \".pgf\"\n",
        "    else:\n",
        "        filename = \"/content/drive/MyDrive/NLP/\" + name + \".pgf\"\n",
        "    plt.savefig(filename, transparent=False, figsize=set_size(\n",
        "        450/2), bbox_inches='tight', dpi=100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def export_test():\n",
        "\n",
        "    matplotlib.use(\"pgf\")\n",
        "    matplotlib.rcParams.update({\n",
        "        \"pgf.texsystem\": \"pdflatex\",\n",
        "        'font.family': 'serif',\n",
        "        'text.usetex': True,\n",
        "        'pgf.rcfonts': False,\n",
        "    })\n",
        "\n",
        "    filename = \"/content/drive/MyDrive/NLP/export_test.csv\"\n",
        "    df = pd.read_csv(filename)\\\n",
        "        .groupby(\"document min\").mean()[\"accuracy\"].to_frame()\\\n",
        "        .rename(columns={\"accuracy\": \"quotes\"})\n",
        "    plt.figure(figsize=set_size(455/2))\n",
        "    df.plot.line()\n",
        "\n",
        "    plt.savefig(\"/content/drive/MyDrive/NLP/export_test_file.pgf\",\n",
        "                transparent=False, figsize=set_size(450/2), bbox_inches='tight', dpi=100)\n",
        "\n",
        "\n",
        "def analysis():\n",
        "\n",
        "    # test how number of authors in training set affect accuracy\n",
        "\n",
        "    # filenames\n",
        "    author_filenames = [\"/content/drive/MyDrive/NLP/auth_count_acc_relation_normal.csv\",\n",
        "                        \"/content/drive/MyDrive/NLP/auth_count_acc_relation_quote_open.csv\",\n",
        "                        \"/content/drive/MyDrive/NLP/auth_count_acc_relation_quotes.csv\",\n",
        "                        \"/content/drive/MyDrive/NLP/auth_count_acc_relation_ref.csv\",\n",
        "                        \"/content/drive/MyDrive/NLP/auth_count_acc_relation_ref_quotes.csv\"]\n",
        "    types = [\"normal\", \"quotesopen\", \"quotes\", \"scripref\", \"ref_quotes\"]\n",
        "\n",
        "    # get data frames for each file\n",
        "    dfs = []\n",
        "    for i, filename in enumerate(author_filenames):\n",
        "        df = pd.read_csv(filename)\\\n",
        "            .groupby(\"author count\").mean()[\"accuracy\"].to_frame()\\\n",
        "            .rename(columns={\"accuracy\": types[i]})\n",
        "        dfs.append(df)\n",
        "\n",
        "    # condense dataframes into one dataframe\n",
        "    df_authors = pd.concat(dfs, axis=1, join=\"inner\")\n",
        "\n",
        "    # show the head of the data frame and plot the whole dataframe\n",
        "    print(df_authors.head())\n",
        "    plt.figure()\n",
        "    lines = df_authors.plot.line()\n",
        "    export_plot(plt, \"auth_count_acc_relation\")\n",
        "    #plt.savefig(str(\"/content/drive/MyDrive/NLP/auth_count_acc_relation.png\"), transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "    # split up the plot into five sections and plot those\n",
        "    n = len(df_authors.index)\n",
        "    s = n // 4\n",
        "    split_df_authors = []\n",
        "    for i in range(0, n, s):\n",
        "        split_df_authors.append(df_authors.iloc[i:i+s, :])\n",
        "    for i, df in enumerate(split_df_authors):\n",
        "        plt.figure()\n",
        "        df.plot.line()\n",
        "        name_len = len(author_filenames[i])\n",
        "        name = author_filenames[i][:name_len-4]+\".png\"\n",
        "        export_plot(plt, name[:len(name)-4], full_address=True)\n",
        "        #plt.savefig(name, transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "    # test how discarding less frequent authors affects accuracy\n",
        "\n",
        "    # filenames\n",
        "    doc_filenames = [\"/content/drive/MyDrive/NLP/doc_acc_relation_normal.csv\",\n",
        "                     \"/content/drive/MyDrive/NLP/doc_acc_relation_quote_open.csv\",\n",
        "                     \"/content/drive/MyDrive/NLP/doc_acc_relation_quotes.csv\",\n",
        "                     \"/content/drive/MyDrive/NLP/doc_acc_relation_ref.csv\",\n",
        "                     \"/content/drive/MyDrive/NLP/doc_acc_relation_ref_quotes.csv\"]\n",
        "\n",
        "    # get data frames for each file\n",
        "    dfs = []\n",
        "    for i, filename in enumerate(doc_filenames):\n",
        "        df = pd.read_csv(filename)\\\n",
        "            .groupby(\"document min\").mean()[\"accuracy\"].to_frame()\\\n",
        "            .rename(columns={\"accuracy\": types[i]})\n",
        "        dfs.append(df)\n",
        "\n",
        "    # condense dataframes into one dataframe\n",
        "    df_docs = pd.concat(dfs, axis=1, join=\"inner\")\n",
        "\n",
        "    # show the head of the data frame and plot the whole dataframe\n",
        "    print(df_docs.head())\n",
        "    plt.figure()\n",
        "    lines = df_docs.plot.line()\n",
        "    export_plot(plt, \"doc_acc_relation\")\n",
        "    #plt.savefig(str(\"/content/drive/MyDrive/NLP/doc_acc_relation.png\"), transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "    # split up the plot into five sections and plot those\n",
        "    n = len(df_docs.index)\n",
        "    s = n // 4\n",
        "    split_df_docs = []\n",
        "    for i in range(0, n, s):\n",
        "        split_df_docs.append(df_docs.iloc[i:i+s, :])\n",
        "\n",
        "    for i, df in enumerate(split_df_docs):\n",
        "        plt.figure()\n",
        "        df.plot.line()\n",
        "        name_len = len(doc_filenames[i])\n",
        "        name = doc_filenames[i][:name_len-4]+\".png\"\n",
        "        export_plot(plt, name[:len(name)-4], full_address=True)\n",
        "        #plt.savefig(name, transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "\n",
        "def plot_with_line(x, y):\n",
        "\n",
        "    x_name = x.name\n",
        "    y_name = y.name\n",
        "\n",
        "    x = x.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "\n",
        "    x = x[:, np.newaxis]\n",
        "    y = y[:, np.newaxis]\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=2)\n",
        "    x_poly = polynomial_features.fit_transform(x)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(x_poly, y)\n",
        "    y_poly_pred = model.predict(x_poly)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_poly_pred))\n",
        "    r2 = r2_score(y, y_poly_pred)\n",
        "    # print(rmse)\n",
        "    # print(r2)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(x, y, s=10)\n",
        "    # sort the values of x before line plot\n",
        "    sort_axis = operator.itemgetter(0)\n",
        "    sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)\n",
        "    x, y_poly_pred = zip(*sorted_zip)\n",
        "    plt.xlabel(x_name)\n",
        "    plt.ylabel(y_name)\n",
        "    plt.plot(x, y_poly_pred, color='g')\n",
        "    export_plot(plt, str(x_name + \"-\" + y_name + \" relation\"))\n",
        "\n",
        "\n",
        "def show_prfd_relations():\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/NLP/model_results.csv\")\n",
        "    df = df[(df[\"author\"] != \"accuracy\") & (df[\"author\"] !=\n",
        "                                            \"macro avg\") & (df[\"author\"] != \"weighted avg\")]\n",
        "    df[\"precision\"] = pd.to_numeric(\n",
        "        df[\"precision\"], downcast=\"float\", errors='coerce')\n",
        "    df[\"documents\"] = pd.to_numeric(\n",
        "        df[\"documents\"], downcast=\"integer\", errors='coerce')\n",
        "    df[\"recall\"] = pd.to_numeric(\n",
        "        df[\"recall\"], downcast=\"float\", errors='coerce')\n",
        "    df[\"f1-score\"] = pd.to_numeric(df[\"f1-score\"],\n",
        "                                   downcast=\"float\", errors='coerce')\n",
        "    df[\"recall\"] = pd.to_numeric(\n",
        "        df[\"recall\"], downcast=\"float\", errors='coerce')\n",
        "\n",
        "    plot_with_line(df[\"documents\"], df[\"precision\"])\n",
        "    #df.plot.scatter(x=\"documents\", y=\"precision\")\n",
        "    print(\"correlation coefficient between {}-{} is: {}\".format(\"documents\",\n",
        "          \"precision\", df[\"documents\"].corr(df[\"precision\"])))\n",
        "\n",
        "    plot_with_line(df[\"documents\"], df[\"recall\"])\n",
        "    #df.plot.scatter(x=\"documents\", y=\"recall\")\n",
        "    print(\"correlation coefficient between {}-{} is: {}\".format(\"documents\",\n",
        "          \"recall\", df[\"documents\"].corr(df[\"recall\"])))\n",
        "\n",
        "    plot_with_line(df[\"documents\"], df[\"f1-score\"])\n",
        "    #df.plot.scatter(x=\"documents\", y=\"f1-score\")\n",
        "    print(\"correlation coefficient between {}-{} is: {}\".format(\"documents\",\n",
        "          \"f1-score\", df[\"documents\"].corr(df[\"f1-score\"])))\n",
        "\n",
        "\n",
        "# export_test()\n",
        "# analysis()\n",
        "show_prfd_relations()\n"
      ],
      "metadata": {
        "id": "1vRjUc4sFAMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "b06f499d-46b8-4f99-cf68-6c491f3153f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correlation coefficient between documents-precision is: -0.35813566006372377\n",
            "correlation coefficient between documents-recall is: 0.6661082203133981\n",
            "correlation coefficient between documents-f1-score is: 0.48165975819693846\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate and save some word clous for each of the authors."
      ],
      "metadata": {
        "id": "n_wH8Qys69qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def generateWordClouds():\n",
        "\n",
        "    authors, texts, author_texts = getAuthorsAndTexts()\n",
        "\n",
        "    condensed_author_texts = {}\n",
        "    for author in author_texts:\n",
        "        print(author)\n",
        "        texts = author_texts[author]\n",
        "        t = \"\"\n",
        "        for text in texts:\n",
        "            t = t + text + \" \"\n",
        "        condensed_author_texts[author] = t\n",
        "\n",
        "    favs = []\n",
        "    with open(\"/content/drive/MyDrive/NLP/fav_speakers.txt\") as file:\n",
        "        content = file.readlines()\n",
        "        content = [line.strip() for line in content]\n",
        "        favs = deepcopy(content)\n",
        "\n",
        "    for author in condensed_author_texts:\n",
        "        text = condensed_author_texts[author]\n",
        "        discards = [\"quoteopen\", \"quoteclose\",\n",
        "                    \"scripref\", \"lord\", \"god\", \"say\"]\n",
        "        text = [word for word in text.split() if word not in discards]\n",
        "        text = \" \".join(text)\n",
        "        wordCloud = WordCloud().generate(text)\n",
        "        plt.title(author)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(wordCloud, interpolation='bilinear')\n",
        "        #plt.savefig(str(\"/content/drive/MyDrive/NLP/speakerWordClouds/\"+author+\"_trans.png\"), transparent=True, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "        plt.savefig(str(\"/content/drive/MyDrive/NLP/speakerWordClouds/\"+author+\".png\"),\n",
        "                    transparent=False, bbox_inches='tight', figsize=(100, 60), dpi=100)\n",
        "        if author in favs:\n",
        "            #plt.savefig(str(\"/content/drive/MyDrive/NLP/favs/\"+author+\"_trans.png\"), bbox_inches='tight', transparent=True, figsize=(100,60), dpi=100)\n",
        "            plt.savefig(str(\"/content/drive/MyDrive/NLP/favs/\"+author+\".png\"),\n",
        "                        bbox_inches='tight', transparent=False, figsize=(100, 60), dpi=100)\n",
        "        # plt.show()\n"
      ],
      "metadata": {
        "id": "0GEKyb1yr79c"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}