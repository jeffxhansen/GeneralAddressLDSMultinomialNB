{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAAI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQy9G0okYsj"
      },
      "source": [
        "Various imports and downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmlikngWfnFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdcd25e-c0d4-43a7-c992-4915bc60cca7"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "\n",
        "# increase the maxInt value\n",
        "maxInt = sys.maxsize\n",
        "while True:\n",
        "    try:\n",
        "        csv.field_size_limit(maxInt)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        maxInt = int(maxInt/10)\n",
        "\n",
        "import string\n",
        "import statistics\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "!pip install Unidecode\n",
        "import unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions used for text proecessing, file creation, and file updating. The majority of this utilizes the python regex library, csv writers and readers, and unidecode to convert the text to utf-8."
      ],
      "metadata": {
        "id": "PFaT_tqUm6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessAuthor(author, speakers, offices, discards, cap=True):\n",
        "  # get rid of ending whitespace\n",
        "  author = author.strip()\n",
        "\n",
        "  # check if not a talk\n",
        "  for d in discards:\n",
        "    if d.lower() in author.lower():\n",
        "      return \"\"\n",
        "\n",
        "  # get just author name without added titles\n",
        "  for o in offices:\n",
        "    if o in author:\n",
        "      regex = re.compile(\".*\"+o+\"[.]?\\s?\")\n",
        "      author = re.sub(regex, \"\", author)\n",
        "\n",
        "  # check if speaker has enough talks\n",
        "  if cap:\n",
        "    viableSpeaker = False\n",
        "    for s in speakers:\n",
        "      if s == author:\n",
        "        viableSpeaker = True\n",
        "        break\n",
        "    if not viableSpeaker:\n",
        "      return \"\"\n",
        "\n",
        "  # remove any ending punctuation if not an abbreviation like Jr.\n",
        "  author_lst = author.split()\n",
        "  if len(author_lst) > 1:\n",
        "    last = author_lst[-1]\n",
        "  elif len(author_lst) == 1:\n",
        "    last = author_lst[0]\n",
        "  else:\n",
        "    return \"\"\n",
        "  if len(last) > 3 and last[-1] in string.punctuation:\n",
        "    author = author[:len(author)-1]\n",
        "  # return the processed author name\n",
        "  return author\n",
        "\n",
        "def normalizeText(text):\n",
        "  # get rid of punctuation\n",
        "  nopunct = [char for char in text if char not in string.punctuation]\n",
        "  nopunct = \"\".join(nopunct)\n",
        "\n",
        "  # lemmatize\n",
        "  lemmatiser = WordNetLemmatizer()\n",
        "  a=\"\"\n",
        "  words = nopunct.split()\n",
        "  for i in range(len(words)):\n",
        "    b = lemmatiser.lemmatize(words[i], pos=\"v\")\n",
        "    b = b.lower()\n",
        "    a = a + b + \" \"\n",
        "\n",
        "  # remove Stopwords\n",
        "  words = [word for word in a.split() if word.lower() not in stopwords.words('english')]\n",
        "  text = \" \".join(words)\n",
        "\n",
        "  return text\n",
        "\n",
        "def preprocessText(text, author, decade, year, TEXT_LEN_CAP, normalize=True):\n",
        "\n",
        "  words = text.split()\n",
        "  if len(words) < TEXT_LEN_CAP:\n",
        "    return \"\"\n",
        "\n",
        "  test_text = text[:600]\n",
        "\n",
        "  change_made = False\n",
        "\n",
        "  # change the text to a normal utf-8 format\n",
        "  text = unidecode.unidecode(text)\n",
        "\n",
        "  # get rid of extra info before actual talk\n",
        "  regex = re.compile(r\"^COMMENTS. \")\n",
        "  match_obj = re.search(regex, test_text[:50])\n",
        "  if match_obj != None:\n",
        "    change_made = True\n",
        "    span = match_obj.span()\n",
        "    text = text[span[1]:]\n",
        "\n",
        "  regex = re.compile(r\"^REMARKS. \")\n",
        "  match_obj = re.search(regex, test_text[:50])\n",
        "  if match_obj != None:\n",
        "    change_made = True\n",
        "    span = match_obj.span()\n",
        "    text = text[span[1]:]\n",
        "\n",
        "  dec_int = int(decade[:len(decade)-1])\n",
        "  year = 0 if year == \"\" else int(year)\n",
        "  if dec_int <= 1880 and (year == \"\" or year < 1880):\n",
        "    regex = re.compile(rf\"(R|r)eported by .*?  \")\n",
        "    match_obj = re.search(regex, test_text)\n",
        "    if match_obj != None:\n",
        "      change_made = True\n",
        "      span = match_obj.span()\n",
        "      text = text[span[1]:]\n",
        "\n",
        "  elif year >= 1971:\n",
        "    regex = re.compile(r\"(W|w)atch, listen, download.\\s*\")\n",
        "    match_obj = re.search(regex, test_text)\n",
        "    if match_obj != None:\n",
        "      change_made = True\n",
        "      span = match_obj.span()\n",
        "      text = text[span[1]:]\n",
        "\n",
        "  else:\n",
        "    regex = re.compile(r\".*as follows:  \")\n",
        "    match_obj = re.search(regex, test_text)\n",
        "    if match_obj != None:\n",
        "      change_made = True\n",
        "      span = match_obj.span()\n",
        "      text = text[span[1]:]\n",
        "\n",
        "    author = author.lower()\n",
        "    counselors = \"|((F|f)irst)|((S|s)econd) counselor\"\n",
        "    remarks = \"|((O|o)pening)|((C|c)losing) remarks\"\n",
        "    options = \"(O|o)f the |(A|a)ssistant\" + counselors + \"|(P|p)resid\" + remarks + \"|(P|p)atriarch\"\n",
        "    regex = re.compile(rf\"({author}(:|\\.)?  (?!{options}))|({author}(:|\\.)?  .*?  )\")\n",
        "    match_obj = re.search(regex, test_text.lower())\n",
        "    if match_obj != None:\n",
        "      change_made = True\n",
        "      span = match_obj.span()\n",
        "      text = text[span[1]:]\n",
        "\n",
        "  # if after removing extra info, too small, reject document\n",
        "  words = text.split()\n",
        "  if len(words) < TEXT_LEN_CAP:\n",
        "    return \"\"\n",
        "\n",
        "  # if no change made, not in a good enough format to use\n",
        "  if not change_made:\n",
        "    return \"\"\n",
        "\n",
        "  # change hyphens to spaces\n",
        "  text = re.sub(\"-\", \" \", text)\n",
        "\n",
        "  # create tokens that represent open quote and close quote\n",
        "  text = re.sub(r'\\s+\"', \" QUOTEOPEN \", text)\n",
        "  text = re.sub(r'\"\\s+', \" QUOTECLOSE \", text)\n",
        "\n",
        "  # replace all scripture references with token \"SCRIPREF\"\n",
        "  regex1 = re.compile(r\"(\\([^\\)]*?:[^\\)]*?\\))|(\\[[^\\]]*?:[^\\]]*?\\])\")\n",
        "  text = re.sub(regex1, \" SCRIPREF \", text)\n",
        "\n",
        "  # get rid of all other paranthetical extras\n",
        "  text = re.sub(r\"\\([^\\)]*?\\)\", \" \", text)\n",
        "\n",
        "  # get rid of stopwords and punctuation, lemmatize the words\n",
        "  if normalize:\n",
        "    text = normalizeText(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def find_text_cap():\n",
        "  text_lengths.sort()\n",
        "  med = statistics.median(text_lengths)\n",
        "  sd = statistics.stdev(text_lengths)\n",
        "  print(med)\n",
        "  print(sd)\n",
        "  print(med-sd)\n",
        "\n",
        "  from matplotlib import pyplot as plt\n",
        "\n",
        "  print(len(text_lengths))\n",
        "  plt.plot(text_lengths[:6000])\n",
        "\n",
        "  Q1 = np.percentile(text_lengths, 25, interpolation = 'midpoint')\n",
        "  Q3 = np.percentile(text_lengths, 75, interpolation = 'midpoint')\n",
        "  IQR = Q3 - Q1\n",
        "  print(\"Q1: \", Q1)\n",
        "  print(\"Q3: \", Q3)\n",
        "  print(\"IQR: \", IQR)\n",
        "  k = 1.5\n",
        "  lower_cap = Q1-k*IQR\n",
        "  arr = np.array(text_lengths)\n",
        "  small = arr[arr < 1578.5]\n",
        "  small_q1 = np.percentile(small, 25, interpolation=\"midpoint\")\n",
        "  print(small_q1)\n",
        "\n",
        "def initFile():\n",
        "  # if it doesn't already exist create the fil; otherwise, clear the file\n",
        "  file = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "\n",
        "  file = open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "\n",
        "  # add headers to the file\n",
        "  with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"r+\") as file:\n",
        "    writer = csv.writer(file, delimiter=\"\\t\")\n",
        "    response = [\"author\", \"text\"]\n",
        "    writer.writerow(response)\n",
        "\n",
        "def addDocToFile(author, text):\n",
        "  # add a row to the newTexts.tsv file with the author and talk\n",
        "  with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"a+\") as file:\n",
        "    writer = csv.writer(file, delimiter=\"\\t\")\n",
        "    lst = []\n",
        "    lst.append(author)\n",
        "    lst.append(text)\n",
        "    writer.writerow(lst)\n",
        "\n",
        "def writeToFile(authors, texts):\n",
        "  # open/create/clear file\n",
        "  file = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "\n",
        "  # add all authors and texts to the file\n",
        "  with open(\"/content/drive/MyDrive/NLP/newTexts.tsv\", \"r+\") as file:\n",
        "    writer = csv.writer(file, delimiter=\"\\t\")\n",
        "    response = [\"author\", \"text\"]\n",
        "    writer.writerow(response)\n",
        "    for i in range(len(authors)):\n",
        "      lst = []\n",
        "      lst.append(authors[i])\n",
        "      lst.append(texts[i])\n",
        "      writer.writerow(lst)\n"
      ],
      "metadata": {
        "id": "H7Rc0Kb-lO3c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the new preprocessed corpus that will be used to train the sklearn multinomial naive bayes classifier. A portion of this code also extracts all of the speakers in the corpus and stores this list in a file. This file is later used to explore how author document count affects the overall model accuracy. This code only needs to be run once, since it saves the processed text in Google Drive.\n",
        "\n",
        "This preprocessing took many hours, so it has been restructured to update the files progressively and in segments if needed."
      ],
      "metadata": {
        "id": "L56ApZVVnRBZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FAhvEzJkcjz"
      },
      "source": [
        "# open the file \n",
        "f = open(\"/content/drive/MyDrive/NLP/restructured-conference.tsv\")\n",
        "f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "# get headers\n",
        "headers = next(f_reader)\n",
        "# discard the open line\n",
        "next(f_reader)\n",
        "\n",
        "TEXT_LEN_CAP = 600\n",
        "\n",
        "with open(\"/content/drive/MyDrive/NLP/speakers.txt\") as file:\n",
        "  speakers = [s.strip() for s in file.readlines()]\n",
        "with open(\"/content/drive/MyDrive/NLP/titles.txt\") as file:\n",
        "  offices = [o.strip() for o in file.readlines()]\n",
        "with open(\"/content/drive/MyDrive/NLP/discards.txt\") as file:\n",
        "  discards = [d.strip() for d in file.readlines()]\n",
        "\n",
        "texts = []\n",
        "authors = []\n",
        "author_texts = {}\n",
        "authors_set = set()\n",
        "text_lengths = []\n",
        "\n",
        "DOC_INDEX_START = 0\n",
        "\n",
        "def init_speaker_count():\n",
        "  file = open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "\n",
        "  '''\n",
        "  file = open(\"/content/drive/MyDrive/NLP/speakers.txt\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "  '''\n",
        "\n",
        "  file = open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\")\n",
        "  file.truncate(0)\n",
        "  file.close()\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"r+\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    response = [\"speaker\", \"count\"]\n",
        "    writer.writerow(response)\n",
        "\n",
        "def add_speaker(speaker:str):\n",
        "  with open(\"/content/drive/MyDrive/NLP/speakers.txt\", \"a+\") as file:\n",
        "    file.write(speaker + \"\\n\")\n",
        "\n",
        "def get_speaker_count():\n",
        "  speaker_counts = {}\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\") as file:\n",
        "    file_reader = csv.reader(file)\n",
        "    next(file_reader)\n",
        "    for row in file_reader:\n",
        "      s = row[0]\n",
        "      c = row[1]\n",
        "\n",
        "      speaker_counts[s] = int(c)\n",
        "\n",
        "  return speaker_counts\n",
        "\n",
        "def create_author_doc_count_file():\n",
        "  speaker_counts = {}\n",
        "  with open(\"/content/drive/MyDrive/NLP/speakers.txt\") as file:\n",
        "    speakers = [speaker.strip() for speaker in file.readlines()]\n",
        "    for s in speakers:\n",
        "      if s not in speaker_counts:\n",
        "        speaker_counts[s] = 1\n",
        "      else:\n",
        "        speaker_counts[s] += 1\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/speaker_counts.csv\", \"r+\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    for s in speaker_counts:\n",
        "      response = [s, speaker_counts[s]]\n",
        "      writer.writerow(response)\n",
        "\n",
        "def create_speaker_file():\n",
        "  f = open(\"/content/drive/MyDrive/NLP/restructured-conference.tsv\")\n",
        "  f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "  # get headers\n",
        "  headers = next(f_reader)\n",
        "  # discard the open line\n",
        "  next(f_reader)\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/docIndex.txt\") as file:\n",
        "    DOC_INDEX_START = file.read()\n",
        "    print(f\"|{DOC_INDEX_START}|\")\n",
        "    if DOC_INDEX_START == \"\":\n",
        "      DOC_INDEX_START = 0\n",
        "    else:\n",
        "      DOC_INDEX_START = int(DOC_INDEX_START)\n",
        "  \n",
        "  doc_index = 0\n",
        "\n",
        "  for document in f_reader:\n",
        "    decade = document[0]\n",
        "    year = document[1]\n",
        "    month = document[2]\n",
        "    session_num = document[3]\n",
        "    author = document[4]\n",
        "    title = document[5]\n",
        "    gender = document[6]\n",
        "    role = document[7]\n",
        "    subjects = document[8]\n",
        "    text = document[9]\n",
        "\n",
        "    doc_index += 1\n",
        "    \n",
        "    # startup where it left off\n",
        "    if doc_index <= DOC_INDEX_START:\n",
        "      continue\n",
        "\n",
        "    author = preprocessAuthor(author, speakers, offices, discards, cap=False)\n",
        "    if author == \"\":\n",
        "      continue\n",
        "    cpy = text\n",
        "    text = preprocessText(text, author, decade, year, TEXT_LEN_CAP, normalize=False)\n",
        "    if text == \"\":\n",
        "      continue\n",
        "    \n",
        "    add_speaker(author)\n",
        "    #print(\"doc:\", doc_index, \"was added\")\n",
        "    with open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\") as file:\n",
        "      file.write(str(doc_index))\n",
        "\n",
        "\n",
        "def select_authors_and_docs():\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/docIndex.txt\") as file:\n",
        "    DOC_INDEX_START = file.read()\n",
        "    print(f\"|{DOC_INDEX_START}|\")\n",
        "    if DOC_INDEX_START == \"\":\n",
        "      DOC_INDEX_START = 0\n",
        "    else:\n",
        "      DOC_INDEX_START = int(DOC_INDEX_START)\n",
        "\n",
        "  print(\"Start doc: \", DOC_INDEX_START)\n",
        "\n",
        "  doc_index = 0\n",
        "\n",
        "  for document in f_reader:\n",
        "    decade = document[0]\n",
        "    year = document[1]\n",
        "    month = document[2]\n",
        "    session_num = document[3]\n",
        "    author = document[4]\n",
        "    title = document[5]\n",
        "    gender = document[6]\n",
        "    role = document[7]\n",
        "    subjects = document[8]\n",
        "    text = document[9]\n",
        "\n",
        "    doc_index += 1\n",
        "    \n",
        "    # startup where it left off\n",
        "    if doc_index <= DOC_INDEX_START:\n",
        "      continue\n",
        "\n",
        "    author = preprocessAuthor(author, speakers, offices, discards)\n",
        "    if author == \"\":\n",
        "      continue\n",
        "    cpy = text\n",
        "    text = preprocessText(text, author, decade, year, TEXT_LEN_CAP)\n",
        "    if text == \"\":\n",
        "      continue\n",
        "\n",
        "    authors.append(author)\n",
        "    texts.append(text)\n",
        "    if author not in author_texts:\n",
        "      author_texts[author] = [text]\n",
        "      text_lengths.append(len(text.split()))\n",
        "    else:\n",
        "      author_texts[author].append(text)\n",
        "      text_lengths.append(len(text.split()))\n",
        "\n",
        "    addDocToFile(author, text)\n",
        "    #print(\"doc:\", doc_index, \"was added\")\n",
        "    with open(\"/content/drive/MyDrive/NLP/docIndex.txt\", \"w+\") as file:\n",
        "      file.write(str(doc_index))\n",
        "\n",
        "def get_speakers(cap=30):\n",
        "  speaker_counts = get_speaker_count()\n",
        "  speakers = []\n",
        "  for s in speaker_counts:\n",
        "    if speaker_counts[s] >= cap:\n",
        "      speakers.append(s)\n",
        "  return speakers\n",
        "\n",
        "#init_speaker_count()\n",
        "#create_author_doc_count_file()\n",
        "#speaker_counts = get_speaker_count()\n",
        "\n",
        "#initFile()\n",
        "#select_authors_and_docs()\n",
        "#timePreprocess()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Multinomial Naive Bayes classifier on the preprocessed text"
      ],
      "metadata": {
        "id": "3c1_2aSCrIWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and downloads\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# download these if skipping the previous cells in the notebook\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "PCoU_-zwwzPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bb1db3-66a1-4bc9-adcf-857e326adcfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to get the necessary authors and texts lists to train the model. These functions are utilized to change up what authors, texts, and the preprocessing of the texts for each model test."
      ],
      "metadata": {
        "id": "qZLHEGtSn6bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getAuthorsAndTexts():\n",
        "  f = open(\"/content/drive/MyDrive/NLP/newTexts.tsv\")\n",
        "  f_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
        "  # get headers\n",
        "  headers = next(f_reader)\n",
        "\n",
        "  authors = []\n",
        "  texts = []\n",
        "  author_text_dict = {}\n",
        "  author_doc_count = {}\n",
        "  author_total_words = {}\n",
        "\n",
        "  for document in f_reader:\n",
        "    author = document[0]\n",
        "    text = document[1]\n",
        "    authors.append(author)\n",
        "    texts.append(text)\n",
        "\n",
        "    if author not in author_text_dict:\n",
        "      author_text_dict[author] = [text]\n",
        "      author_doc_count[author] = 1\n",
        "      author_total_words[author] = len(text.split())\n",
        "    else:\n",
        "      author_text_dict[author].append(text)\n",
        "      author_doc_count[author] += 1\n",
        "      author_total_words[author] += len(text.split())\n",
        "\n",
        "  return authors, texts, author_text_dict, author_doc_count, author_total_words\n",
        "\n",
        "def text_process(text):\n",
        "  return [word for word in text.split()]\n",
        "\n",
        "def text_process_discards(text, discards=None):\n",
        "  discards = [\"quoteopen\", \"quoteclose\", \"scripref\", \"lord\", \"god\", \"say\"] if discards == None else discards\n",
        "  return [word for word in text.split() if word not in discards]\n",
        "\n",
        "def discard(texts, discards):\n",
        "  for i in range(len(texts)):\n",
        "    text = texts[i]\n",
        "    text = \" \".join([word for word in text.split() if word not in discards])\n",
        "    texts[i] = text\n",
        "  return texts"
      ],
      "metadata": {
        "id": "XEVMK7FywwJi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code used to train the multinomial naive bayes model and run various tests. The tests focus on how the document cap (getting rid of authors who have fewer documents attributed to them), the number of authors trained together (batch size), and how features like quotes and scripture references affect the overall model accuracy."
      ],
      "metadata": {
        "id": "1WdSv7GtoNOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import exit\n",
        "from numpy.testing import assert_equal\n",
        "from statistics import mean\n",
        "from copy import deepcopy\n",
        "from random import randint\n",
        "\n",
        "authors, texts, author_texts, author_doc_count, author_total_words = getAuthorsAndTexts()\n",
        "\n",
        "'''\n",
        "counter = 0\n",
        "for i in range(len(y)):\n",
        "  print(y[i], labels[i], classes[y[i]])\n",
        "  counter += 1\n",
        "  if counter > 20:\n",
        "    break\n",
        "\n",
        "#exit()\n",
        "'''\n",
        "def filter_by_authors(auth_lst):\n",
        "  global authors\n",
        "  global texts\n",
        "  global author_texts\n",
        "  global author_doc_count\n",
        "  global author_total_words\n",
        "\n",
        "  authors_new = []\n",
        "  texts_new = []\n",
        "  auths = set()\n",
        "  for i, a in enumerate(authors):\n",
        "    if a in auth_lst:\n",
        "      authors_new.append(a)\n",
        "      texts_new.append(texts[i])\n",
        "      auths.add(a)\n",
        "\n",
        "  return authors_new, texts_new, len(auths)\n",
        "\n",
        "def filter_author_count(count = 50):\n",
        "  global authors\n",
        "  global texts\n",
        "  global author_texts\n",
        "  global author_doc_count\n",
        "  global author_total_words\n",
        "\n",
        "  authors_new = []\n",
        "  texts_new = []\n",
        "  auths = set()\n",
        "  for i, a in enumerate(authors):\n",
        "    if author_doc_count[a] > count:\n",
        "      authors_new.append(a)\n",
        "      texts_new.append(texts[i])\n",
        "      auths.add(a)\n",
        "\n",
        "  return authors_new, texts_new, len(auths)\n",
        "\n",
        "def mnb_model(authors, texts, discards=[], print_report = True):\n",
        "\n",
        "  authors = deepcopy(authors)\n",
        "  texts = deepcopy(texts)\n",
        "\n",
        "  labelencoder = LabelEncoder()\n",
        "  y = labelencoder.fit_transform(authors)\n",
        "  authors_reverse = labelencoder.inverse_transform(y)\n",
        "  labels = list(labelencoder.classes_)\n",
        "\n",
        "  y_counts = {}\n",
        "  for e in y:\n",
        "    if e not in y_counts:\n",
        "      y_counts[e] = 1\n",
        "    else:\n",
        "      y_counts[e] += 1\n",
        "\n",
        "  author_word_count = {}\n",
        "  for i, a in enumerate(authors):\n",
        "    words = len(texts[i].split())\n",
        "    if a not in author_word_count:\n",
        "      author_word_count[a] = words\n",
        "    else:\n",
        "      author_word_count[a] += words\n",
        "\n",
        "  texts = discard(texts, discards)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "  bow_transformer = CountVectorizer().fit(X_train)\n",
        "  text_bow_train = bow_transformer.transform(X_train)\n",
        "  text_bow_test = bow_transformer.transform(X_test)\n",
        "\n",
        "  model = MultinomialNB()\n",
        "  model = model.fit(text_bow_train, y_train)\n",
        "\n",
        "  model.score(text_bow_train, y_train)\n",
        "  model.score(text_bow_test, y_test)\n",
        "\n",
        "  predictions = model.predict(text_bow_test)\n",
        "  report = classification_report(y_test, predictions, zero_division=0, output_dict=True)\n",
        "  report_str = classification_report(y_test, predictions, zero_division=0, output_dict=False)\n",
        "  if print_report:\n",
        "    min = np.inf\n",
        "    max = 0\n",
        "    mins = []\n",
        "    wpds = []\n",
        "    print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"\", \"precision\", \"recall\", \"f1-score\", \"documents\", \"words\", \"wpd\"))\n",
        "    for item in report:\n",
        "      #print(item)\n",
        "      if item == \"accuracy\":\n",
        "        print()\n",
        "        l = \"accuracy\"\n",
        "        p = \" \"\n",
        "        r = \" \"\n",
        "        f = round(report[item],2)\n",
        "        d = \" \"\n",
        "        w = \" \"\n",
        "        wpd = \" \"\n",
        "      elif item == \"micro avg\":\n",
        "        l = \"micro avg\"\n",
        "        p = round(report[item][\"precision\"],2)\n",
        "        r = round(report[item][\"recall\"],2)\n",
        "        f = round(report[item][\"f1-score\"],2)\n",
        "        d = \" \"\n",
        "        w = \" \"\n",
        "        wpd = \" \"\n",
        "      elif item == \"weighted avg\":\n",
        "        l = \"weighted avg\"\n",
        "        p = round(report[item][\"precision\"],2)\n",
        "        r = round(report[item][\"recall\"],2)\n",
        "        f = round(report[item][\"f1-score\"],2)\n",
        "        d = \" \"\n",
        "        w = \" \"\n",
        "        wpd = \" \"\n",
        "      elif item == \"macro avg\":\n",
        "        l = \"macro avg\"\n",
        "        p = round(report[item][\"precision\"],2)\n",
        "        r = round(report[item][\"recall\"],2)\n",
        "        f = round(report[item][\"f1-score\"],2)\n",
        "        d = \" \"\n",
        "        w = \" \"\n",
        "        wpd = \" \"\n",
        "      else:\n",
        "        l = labels[int(item)]\n",
        "        p = round(report[item][\"precision\"],2)\n",
        "        r = round(report[item][\"recall\"],2)\n",
        "        f = round(report[item][\"f1-score\"],2)\n",
        "        d = y_counts[int(item)]\n",
        "        w = author_word_count[l]\n",
        "        wpd = w/d\n",
        "\n",
        "        if f == 0.0 and d > max:\n",
        "          max = d\n",
        "        if f > 0 and d < min:\n",
        "          min = d\n",
        "\n",
        "        wpds.append(w/d)\n",
        "\n",
        "\n",
        "      print(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(l, p, r, f, d, w, wpd))\n",
        "    print(\"minimum documents to produce results was min: {} max: {}\".format(min, max))\n",
        "    print(\"average word per doc: {}\".format(mean(wpds)))\n",
        "\n",
        "    #print(report_str)\n",
        "\n",
        "  return model, report\n",
        "\n",
        "def run_cap_test(discards=[], label=\"\"):\n",
        "  doc_caps, accuracies, author_counts, remainings = [], [], [], []\n",
        "\n",
        "  authors_set = set()\n",
        "  prev_authors_set = set()\n",
        "\n",
        "  for _ in range(10):\n",
        "    for i in range(25,100,5):\n",
        "      auths_test, texts_test, auths = filter_author_count(i)\n",
        "      authors_set = set(auths_test)\n",
        "      m, r = mnb_model(auths_test, texts_test, discards=discards, print_report=False)\n",
        "      remaining = prev_authors_set - authors_set\n",
        "      doc_caps.append(i)\n",
        "      accuracies.append(r[\"accuracy\"])\n",
        "      author_counts.append(auths)\n",
        "      remainings.append(remaining)\n",
        "      prev_authors_set = deepcopy(authors_set)\n",
        "\n",
        "      #print(i, \" : \", r[\"accuracy\"], \" : \", auths, \" : \", remaining)\n",
        "\n",
        "  file = open(\"/content/drive/MyDrive/NLP/doc_acc_relation\"+str(label)+\".csv\", \"w+\")\n",
        "  file.close()\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/doc_acc_relation\"+str(label)+\".csv\", \"r+\") as file:\n",
        "      writer = csv.writer(file)\n",
        "      header = [\"document min\", \"accuracy\", \"number of valid authors\", \"discarded authors\"]\n",
        "      writer.writerow(header)\n",
        "      for i in range(len(doc_caps)):\n",
        "        cap = doc_caps[i]\n",
        "        acc = accuracies[i]\n",
        "        auth = author_counts[i]\n",
        "        rem = remainings[i]\n",
        "\n",
        "        response = [cap, acc, auth, rem]\n",
        "        writer.writerow(response)\n",
        "\n",
        "def author_amount_test(discards=[], label=\"\"):\n",
        "  lst = []\n",
        "  with open(\"/content/drive/MyDrive/NLP/doc_acc_relation\"+str(label)+\".csv\") as file:\n",
        "    f_reader = csv.reader(file)\n",
        "    next(f_reader)\n",
        "    for line in f_reader:\n",
        "      text = line[3]\n",
        "      if text != \"set()\":\n",
        "        lst = lst + text[2:len(text)-2].split(\"', '\")\n",
        "  #print(lst)\n",
        "  #lst = lst[:len(lst)-2]\n",
        "  #print(len(lst))\n",
        "\n",
        "  rows = []\n",
        "  for _ in range(10):\n",
        "    for size in range(4,len(lst),2):\n",
        "      batches = []\n",
        "      total = 0\n",
        "      author_bank = deepcopy(lst)\n",
        "      counter = 0\n",
        "      batch = []\n",
        "      while len(author_bank) > 0:\n",
        "        a = author_bank.pop(randint(0,len(author_bank)-1))\n",
        "        batch.append(a)\n",
        "        counter += 1\n",
        "        if counter >= size:\n",
        "          counter = 0\n",
        "          batches.append(batch)\n",
        "          batch = []\n",
        "      \n",
        "      counts, accuracies, dpa = [], [], []\n",
        "\n",
        "      accuracies = []\n",
        "      reg_size = len(batches[0])\n",
        "      for b in batches:\n",
        "        if len(b) == reg_size:\n",
        "          authors_batch, texts_batch, authors_count = filter_by_authors(b)\n",
        "          m, r = mnb_model(authors_batch, texts_batch, discards=discards, print_report=False)\n",
        "          \n",
        "          counts.append(authors_count)\n",
        "          accuracies.append(r[\"accuracy\"])\n",
        "          dpa.append(len(texts_batch)/authors_count)\n",
        "\n",
        "          rows.append([authors_count, r[\"accuracy\"], len(texts_batch)/authors_count])\n",
        "          #print(authors_count, \" : \", r[\"accuracy\"], \" : \", len(texts_batch)/authors_count, \" : \", set(authors_batch))\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/NLP/auth_count_acc_relation\"+str(label)+\".csv\", \"w+\") as file:\n",
        "    file.truncate(0)\n",
        "  with open(\"/content/drive/MyDrive/NLP/auth_count_acc_relation\"+str(label)+\".csv\", \"r+\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    header = [\"author count\", \"accuracy\", \"avg doc_per_author\"]\n",
        "    writer.writerow(header)\n",
        "    for r in rows:\n",
        "      writer.writerow(r)\n",
        "\n",
        "def discards_test():\n",
        "  discards = [[\"quoteopen\", \"quoteclose\", \"scripref\"], [\"quoteopen\", \"quoteclose\"], [\"scripref\"], [], [\"quoteclose\"]]\n",
        "  labels = [\"normal\", \"ref\", \"quotes\", \"ref_quotes\", \"quote_open\"]\n",
        "\n",
        "  for i, d in enumerate(discards):\n",
        "    authors, texts, auths_count = filter_author_count(60)\n",
        "    run_cap_test(d, str(\"_\"+labels[i]))\n",
        "    author_amount_test(d, str(\"_\"+labels[i]))\n",
        "    \n",
        "discards_test()\n",
        "#author_amount_test()\n",
        "#run_cap_test()\n",
        "#mnb_model(authors, texts)\n",
        "\n",
        "def get_batches(authors, texts, batches=10):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "vGRInv6Z8EoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analsysis and plotting of each of the tests"
      ],
      "metadata": {
        "id": "sN8qEk82E4QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def analysis():\n",
        "\n",
        "  # test how number of authors in training set affect accuracy\n",
        "\n",
        "  # filenames\n",
        "  author_filenames = [\"/content/drive/MyDrive/NLP/auth_count_acc_relation_normal.csv\",\n",
        "                      \"/content/drive/MyDrive/NLP/auth_count_acc_relation_quote_open.csv\",\n",
        "                      \"/content/drive/MyDrive/NLP/auth_count_acc_relation_quotes.csv\",\n",
        "                      \"/content/drive/MyDrive/NLP/auth_count_acc_relation_ref.csv\",\n",
        "                      \"/content/drive/MyDrive/NLP/auth_count_acc_relation_ref_quotes.csv\"]\n",
        "  types = [\"normal\", \"quotesopen\", \"quotes\", \"scripref\", \"ref_quotes\"]\n",
        "\n",
        "  # get data frames for each file\n",
        "  dfs = []\n",
        "  for i, filename in enumerate(author_filenames):\n",
        "    df = pd.read_csv(filename)\\\n",
        "      .groupby(\"author count\").mean()[\"accuracy\"].to_frame()\\\n",
        "      .rename(columns={\"accuracy\":types[i]})\n",
        "    dfs.append(df)\n",
        "\n",
        "  # condense dataframes into one dataframe\n",
        "  df_authors = pd.concat(dfs, axis=1)\n",
        "\n",
        "  # show the head of the data frame and plot the whole dataframe\n",
        "  print(df_authors.head())\n",
        "  plt.figure()\n",
        "  lines = df_authors.plot.line()\n",
        "  plt.savefig(str(\"/content/drive/MyDrive/NLP/auth_count_acc_relation.png\"), transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "  # split up the plot into five sections and plot those\n",
        "  n = len(df_authors.index)\n",
        "  s = n // 4\n",
        "  split_df_authors = []\n",
        "  for i in range(0, n, s):\n",
        "    split_df_authors.append(df_authors.iloc[i:i+s,:])\n",
        "  for i, df in enumerate(split_df_authors):\n",
        "    plt.figure()\n",
        "    df.plot.line()\n",
        "    name_len = len(author_filenames[i])\n",
        "    name = author_filenames[i][:name_len-4]+\".png\"\n",
        "    plt.savefig(name, transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "  # test how discarding less frequent authors affects accuracy\n",
        "\n",
        "  # filenames\n",
        "  doc_filenames = [\"/content/drive/MyDrive/NLP/doc_acc_relation_normal.csv\",\n",
        "                   \"/content/drive/MyDrive/NLP/doc_acc_relation_quote_open.csv\",\n",
        "                   \"/content/drive/MyDrive/NLP/doc_acc_relation_quotes.csv\",\n",
        "                   \"/content/drive/MyDrive/NLP/doc_acc_relation_ref.csv\",\n",
        "                   \"/content/drive/MyDrive/NLP/doc_acc_relation_ref_quotes.csv\"]\n",
        "\n",
        "  # get data frames for each file\n",
        "  dfs = []\n",
        "  for i, filename in enumerate(doc_filenames):\n",
        "    df = pd.read_csv(filename)\\\n",
        "      .groupby(\"document min\").mean()[\"accuracy\"].to_frame()\\\n",
        "      .rename(columns={\"accuracy\":types[i]})\n",
        "    dfs.append(df)\n",
        "\n",
        "  # condense dataframes into one dataframe\n",
        "  df_docs = pd.concat(dfs, axis=1)\n",
        "\n",
        "  # show the head of the data frame and plot the whole dataframe\n",
        "  print(df_docs.head())\n",
        "  plt.figure()\n",
        "  lines = df_docs.plot.line()\n",
        "  plt.savefig(str(\"/content/drive/MyDrive/NLP/doc_acc_relation.png\"), transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "  # split up the plot into five sections and plot those\n",
        "  n = len(df_docs.index)\n",
        "  s = n // 4\n",
        "  split_df_docs = []\n",
        "  for i in range(0, n, s):\n",
        "    split_df_docs.append(df_docs.iloc[i:i+s,:])\n",
        "\n",
        "  # split up the plot into five sections and plot those\n",
        "  for i, df in enumerate(split_df_docs):\n",
        "    plt.figure()\n",
        "    df.plot.line()\n",
        "    name_len = len(doc_filenames[i])\n",
        "    name = doc_filenames[i][:name_len-4]+\".png\"\n",
        "    plt.savefig(name, transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "\n",
        "#analysis()\n"
      ],
      "metadata": {
        "id": "1vRjUc4sFAMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate and save some word clous for each of the authors."
      ],
      "metadata": {
        "id": "n_wH8Qys69qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateWordClouds():\n",
        "\n",
        "  authors, texts, author_texts = getAuthorsAndTexts()\n",
        "\n",
        "  condensed_author_texts = {}\n",
        "  for author in author_texts:\n",
        "    print(author)\n",
        "    texts = author_texts[author]\n",
        "    t = \"\"\n",
        "    for text in texts:\n",
        "      t = t + text + \" \"\n",
        "    condensed_author_texts[author] = t\n",
        "\n",
        "  favs = []\n",
        "  with open(\"/content/drive/MyDrive/NLP/fav_speakers.txt\") as file:\n",
        "    content = file.readlines()\n",
        "    content = [line.strip() for line in content]\n",
        "    favs = deepcopy(content)\n",
        "\n",
        "  for author in condensed_author_texts:\n",
        "    text = condensed_author_texts[author]\n",
        "    discards = [\"quoteopen\", \"quoteclose\", \"scripref\", \"lord\", \"god\", \"say\"]\n",
        "    text = [word for word in text.split() if word not in discards]\n",
        "    text = \" \".join(text)\n",
        "    wordCloud = WordCloud().generate(text)\n",
        "    plt.title(author)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(wordCloud, interpolation='bilinear')\n",
        "    #plt.savefig(str(\"/content/drive/MyDrive/NLP/speakerWordClouds/\"+author+\"_trans.png\"), transparent=True, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "    plt.savefig(str(\"/content/drive/MyDrive/NLP/speakerWordClouds/\"+author+\".png\"), transparent=False, bbox_inches='tight', figsize=(100,60), dpi=100)\n",
        "    if author in favs:\n",
        "      #plt.savefig(str(\"/content/drive/MyDrive/NLP/favs/\"+author+\"_trans.png\"), bbox_inches='tight', transparent=True, figsize=(100,60), dpi=100)\n",
        "      plt.savefig(str(\"/content/drive/MyDrive/NLP/favs/\"+author+\".png\"), bbox_inches='tight', transparent=False, figsize=(100,60), dpi=100)\n",
        "    #plt.show()\n"
      ],
      "metadata": {
        "id": "0GEKyb1yr79c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}